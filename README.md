# PROWN-Bilinear-Attention

ðŸ”’ **Probabilistically Robust Watermarking of Neural Networks Enhanced with Bilinear Pooling and Attention Mechanisms**

This repository provides an extension of the original PROWN (Probabilistically Robust Watermarking of Neural Networks) framework by improving the trigger set generation process. We incorporate **bilinear pooling** and **scaled dot-product attention** to generate semantically meaningful and highly transferable watermark triggers.

> ðŸ“ˆ Improvement of ~34% in verification accuracy with enhanced triggers.

---

## ðŸ§  Key Features

- âœ… Enhanced trigger generation using **bilinear pooling**
- âœ… Feature-guided fusion via **attention mechanisms**
- âœ… Improved **robustness against model stealing** and distillation attacks
- âœ… Minimal computational overhead â€” fully compatible with original PROWN
- âœ… Tested on **CIFAR-10** with **ResNet34 (teacher)** and **VGG11 (student)** models

---

To run this project just run the jupyter notebook `PROWN_CODE_FINAL.ipynb` in the root directory.
